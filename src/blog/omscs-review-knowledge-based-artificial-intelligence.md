---
title: "OMSCS Review: Knowledge-Based Artificial Intelligence"
date: 2023-12-31T22:48:16.618Z
updatedOn: ""
tags:
  - OMSCS
  - AI
draft: true
---
Artificial intelligence is one of the hottest topics within the tech industry, with OpenAI and ChatGPT making headlines even in mainstream news publications. There possibly has never been a more relevant time to take a class on the subject of AI, so this fall I enrolled in Georgia Tech's [CS 7637: Knowledge-Based Artificial Intelligence - Cognitive Systems](https://omscs.gatech.edu/cs-7637-knowledge-based-artificial-intelligence-cognitive-systems) course. I have some previous academic experience with AI from my time as an undergrad at Northwestern, but I was interested in revisiting the subject since so much has changed in the field since then. Somewhat unfortunately, this class focused largely on classic AI techniques and didn't delve too deeply into newer topics such as Large Language Models (LLMs) or other forms of generative AI. Despite being a little outdated, it was interesting thinking about AI approaches modeled after our own human cognition, and this class was as much about understanding human learning as it was machine learning.

CS 7637 featured a fairly rapid pace of coursework, with a new deliverable due every week. The class assignments fell into one of three categories: mini-projects, written homework, and the Raven's Progressive Matrices (RPM) project. 

* The mini-projects were coding challenges that were roughly related to the lecture material accompanied by a written report explaining the approach. Somewhat disappointingly, the automated test-suites for the mini-projects could usually be brute-forced, which lead to rather uninteresting solutions and reports. 
* The written homework was comprised of essays answering a prompt related to one of the lecture topics. I actually enjoyed these assignments for the most part, since they provided an opportunity to creatively apply the course concepts.
* Finally, CS 7637 featured a course-long project that required writing an AI agent that solves Raven's Progressive Matrices problems, which are questions given to assess general intelligence in a non-verbal way similar to an IQ test. This was by far the highlight of the course for me, as I appreciated both the project and the organization of its deliverables. Throughout the course, there were four milestones that assessed my agent's ability to correctly answer different categories of questions, which made it feel like I was making incremental progress towards building a better agent. It was rather difficult to get full marks on this project (or at least it should be, more on that later). Ultimately my agent correctly answered 81/96 questions on the graded test suite, which is a grade that I'm proud of.

In addition to the above assignments, there were two open-book multiple choice exams as well as a class participation requirement.

However, when I look back on this past semester, the defining feature of CS 7637 wasn't the assignments, exams, or other coursework. Instead, I remember all of the drama on the class forum throughout the course. There were several controversial decisions by the teaching staff that received mixed or negative reactions from the student body, which added unnecessary stress and frustration to the course. 

One controversial decision from the teaching staff involved their ruling on an unconventional approach to the RPM project. The project submission was graded against both known and hidden test cases, with the intention of having the hidden test cases validate that the agent's approach was generally applicable across any and all RPM problems. A very clever student discovered a way to harvest the correct answers from autograder and then hardcoded their agent to answer correctly answer all questions. They then publicly posted their approach on the class forum seeking teaching staff approval for this methodology, which resulted in many other students copying the approach. The teaching staff were slow to respond to this discussion, and ultimately ruled that this approach fell under the AI strategy of "learning by recording cases" and was therefore valid. 

Many students felt that this approach violated the spirit of the course, since it did not require any "intelligence" on behalf of the agent. Personally, I don't blame any student who implemented this method in their agent since it was not explicitly denied by the teaching staff. But in my opinion, it does feel pretty unfair because it is very difficult to build an agent that scores highly on the RPM test suite using a generalizable approach. This controversy raises interesting questions about defining intelligence in computer systems: is it just a matter of getting the correct answer by any means, or does the implementation matter? How do we define if an implementation is intelligent?

Overall, I was a little disappointed in CS 7637 since I had pretty high hopes for this course. I enjoyed [CS 6750: Human-Computer Interaction](https://mattdalzell.com/blog/omscs-human-computer-interaction/), another Dr. Joyner-composed class, so I did not expect CS 7637 to be so disorganized. Dr. Joyner was not directly involved in the course this past semester, so perhaps that contributed to some of the communication issues, although I don't think it would've prevented any of the controversy. The class material was perfectly fine, if a little out of date, so hopefully with the teaching staff will apply the lessons learned from this semester in order to improve future iterations of this course.